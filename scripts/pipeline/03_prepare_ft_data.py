"""
Data Preparation for Fine-tuning LCA Embedding Model

修正要点：
- 保留文档 id（dataset_uuid | version）并以此去重 corpus，避免同一文档多 id。
- qrels 用 doc_id 对应 corpus，负采样跳过同 doc_id，避免把正样本当负样本。
- 可选地按 dataset_type 对 flow/process 做 1:1 平衡（flow 数据量过大时使用）。

输出格式（与之前一致）：
- training.json: {"query": str, "pos": List[str], "neg": List[str], "id": str, "prompt": str}
- test_queries.jsonl: {"id": str, "text": str}
- corpus.jsonl: {"id": str, "text": str}  # id 为 doc_id
- test_qrels.jsonl: {"qid": str, "docid": str, "relevance": int}
"""

import argparse
import json
import os

import numpy as np
import pandas as pd
from src.prep.data_prep import (
    balance_sources,
    build_doc_id,
    inject_hard_negatives,
    load_hard_negatives,
    sample_negatives,
    split_and_save,
)

# Default configuration
DATA_PATH = "data/lca_embedding_dataset.csv"
OUTPUT_DIR = "data/ft_data"
NEG_NUM = 10  # Number of negative samples per query
TEST_SIZE = 0.1
RANDOM_SEED = 520
INSTRUCTION = ""  # 留空，搜索场景下不需要额外前缀


def parse_args():
    parser = argparse.ArgumentParser(description="Prepare fine-tuning data for LCA embedding models.")
    parser.add_argument(
        "--data_path",
        type=str,
        nargs="+",
        default=[DATA_PATH],
        help="Path(s) to raw CSV data generated by step2 (supports multiple)",
    )
    parser.add_argument("--output_dir", type=str, default=OUTPUT_DIR, help="Directory to store processed data")
    parser.add_argument("--neg_num", type=int, default=NEG_NUM, help="Random negatives per query")
    parser.add_argument("--test_size", type=float, default=TEST_SIZE, help="Test split ratio")
    parser.add_argument("--random_seed", type=int, default=RANDOM_SEED, help="Random seed")
    parser.add_argument("--instruction", type=str, default=INSTRUCTION, help="Prompt prefix for every sample")
    parser.add_argument("--hard_negatives_path", type=str, default=None, help="Optional JSONL with mined hard negatives")
    parser.add_argument(
        "--source_column",
        type=str,
        default="dataset_type",
        help="Column name to identify data source (used for flow/process balancing)",
    )
    parser.add_argument(
        "--no_source_balance",
        action="store_true",
        help="Disable 1:1 balancing between process/flow rows when source_column is present",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    print("=" * 60)
    print("LCA Embedding Dataset Preparation")
    print("=" * 60)

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Step 1: Load data
    print("\n[1/6] Loading data...")
    data_paths = args.data_path if isinstance(args.data_path, list) else [args.data_path]
    frames = []
    for path in data_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Data path not found: {path}")
        part_df = pd.read_csv(path)
        frames.append(part_df)
        print(f"  - {path}: {len(part_df)} rows | columns: {list(part_df.columns)}")
    df = pd.concat(frames, ignore_index=True) if len(frames) > 1 else frames[0]
    print(f"  - Combined rows: {len(df)} from {len(frames)} file(s)")

    # Step 2: Build doc_id, dedup queries, and keep doc text
    print("\n[2/6] Preparing dataset structure...")
    df = df.rename(
        columns={
            "query": "query",
            "dataset_content": "doc_text",
            "dataset_uuid": "dataset_uuid",
            "dataset_version": "dataset_version",
        }
    )

    df["doc_id"] = df.apply(build_doc_id, axis=1)
    base_cols = ["query", "doc_text", "doc_id"]
    if args.source_column in df.columns:
        base_cols.append(args.source_column)
    df = df[base_cols]

    # 去重：同一个 query/doc_id 只保留一条，防止同文档重复行
    original_len = len(df)
    df = df.drop_duplicates(subset=["query", "doc_id"], keep="first").reset_index(drop=True)
    print(f"  - Removed {original_len - len(df)} duplicate query-doc pairs")
    print(f"  - Unique queries: {df['query'].nunique()}, unique docs: {df['doc_id'].nunique()}")

    # Balance flow / process to 1:1 if enabled
    if args.no_source_balance:
        print("  - Source balancing skipped (disabled)")
    else:
        df = balance_sources(df, args.source_column, args.random_seed)

    if args.source_column in df.columns:
        df = df.drop(columns=[args.source_column])

    # 分配查询 id
    df["id"] = [str(i) for i in range(len(df))]

    # Corpus 去重 + 负采样
    df, doc_df = sample_negatives(df, args.neg_num, args.random_seed)

    # Step 4: Add prompt & hard negatives
    print("\n[4/6] Adding instruction prompt...")
    df["prompt"] = [args.instruction] * len(df)
    print(f"  - Prompt: '{args.instruction}'")

    hard_neg_map = load_hard_negatives(args.hard_negatives_path)
    if hard_neg_map:
        print(f"  - Injecting hard negatives from {args.hard_negatives_path}")
        df = inject_hard_negatives(df, hard_neg_map)

    # Split & save
    print(f"\n[5/6] Splitting dataset (test_size={args.test_size})...")
    split_and_save(df, doc_df, args.output_dir, args.test_size, args.random_seed)

    print("\n" + "=" * 60)
    print("Data preparation completed!")
    print("=" * 60)
    print(f"\nOutput files in '{args.output_dir}/':")
    print(f"  - training.json      : Training data for fine-tuning")
    print(f"  - test_queries.jsonl : Test queries for evaluation")
    print(f"  - corpus.jsonl       : Full corpus for retrieval")
    print(f"  - test_qrels.jsonl   : Query-document relevance labels")

    # Show sample
    print("\n" + "-" * 60)
    print("Sample from training data:")
    print("-" * 60)
    sample = train[0]
    print(f"Query: {sample['query'][:100]}...")
    print(f"Pos (first 100 chars): {sample['pos'][0][:100]}...")
    print(f"Neg count: {len(sample['neg'])}")
    print(f"ID: {sample['id']}")
    print(f"Prompt: {sample['prompt']}")


if __name__ == "__main__":
    main()
