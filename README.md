# LCA Domain Embedding Project

Domain-specific embedding fine-tuning and evaluation for Lifecycle Assessment (LCA) retrieval. This README is for external presentation (no usage steps), summarizing scope, data, models, metrics, and structure.

## Overview
- Goal: build and validate domain embeddings for LCA retrieval, quantifying ranking/recall gains over generic and cloud baselines.
- Deliverables: fine-tuned model, cached embeddings and evaluation outputs, visual reports (EN/ZH).

## Data & Scale
- Source: TianGong LCA, Tidas structured data converted to Markdown; queries generated by LLM.
- Cleaning: unify `doc_id = uuid[|version]`, dedup `(query, doc_id)`, dedup docs; 10 negatives/query (optionally hard negatives); fixed seed split.
- Scale: train 17,037 query-doc; eval 1,893 queries / 3,786 corpus / 1,893 qrels.

## Method & Pipeline (overview)
1) Query/data prep: Tidas → Markdown → queries; dedup, negative sampling, split; export train/eval files.
2) Fine-tuning: Qwen3-Embedding-0.6B, MultipleNegativesRankingLoss, bf16/single or multi-GPU.
3) Embedding cache: one-time encoding of queries/corpus with vectors/IDs/meta.
4) Evaluation: Faiss inner-product Flat Top-100; pytrec_eval for NDCG/MAP/Recall/Precision; MRR computed separately.
5) Reports: key metrics and charts (bar for head metrics, recall curves), HTML/MD in EN & ZH.

## Models
- raw: Qwen3-Embedding-0.6B (generic).
- ft: Qwen3-Embedding-0.6B fine-tuned on LCA domain.
- Cloud baselines: qwen3-embedding-8b / 4b, bge-m3, codestral-embed-2505.

## Metrics & Highlights
- Metrics: NDCG/MAP/Recall/Precision/MRR @ {1,5,10,50,100}, averaged per query.
- Headline: fine-tuned vs raw → NDCG@10 +31.2%, Recall@10 +25.7%, MRR@10 +33.5%; Recall@100 +11.5%. Codestral is the best cloud baseline but still behind ft; others show marginal gains.

## Structure
- scripts/
  - pipeline/: numbered steps 01–07 for data → train → cache → eval
  - reports/: report generators
  - tools/: hard negatives, similarity, model download/upload, conversions
  - legacy/: legacy entrypoints
- src/: reusable modules
  - embed/ (cache/encode), eval/ (metrics), prep/ (data prep), report/ (render), train/, data/
- data/: ft_data, eval_cache, output (metrics, etc.)
- docs/: templates and historical reports; root `report.md` (EN), `report.ZH.md` (ZH)

## Dependencies & Environment
- Key deps: sentence-transformers, faiss, pytrec_eval, datasets, requests, numpy, pandas, tqdm, etc.
- Cloud models: OpenRouter requires `OPENROUTER_API_KEY`; bf16 depends on hardware.
- Run scripts from repo root so `src` is importable.

## License
- MIT (see LICENSE).

## Links & Citation (placeholders)
- arXiv: TBA
 - Citation: TBA
- Hugging Face / Ollama: TBA
